{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting tables\n",
      "  Using cached https://files.pythonhosted.org/packages/b3/b4/a710fad882736dcd42d5e4ac49c953be002408d20981dc2b8a2bc6837729/tables-3.5.2-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting numexpr>=2.6.2 (from tables)\n",
      "  Using cached https://files.pythonhosted.org/packages/a8/49/b27f05c86de31e8db391bf5dee750bfc16d8ffd50b97f72c19b8a2d6211a/numexpr-2.7.1-cp27-cp27m-macosx_10_6_intel.whl\n",
      "Collecting mock>=2.0 (from tables)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.9.3 (from tables)\n",
      "  Using cached https://files.pythonhosted.org/packages/09/96/84cf406fe7d589f3dba9fc0f737e65985a3526c6d8c783f02d4b5a10825d/numpy-1.16.6-cp27-cp27m-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from tables) (1.12.0)\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0->tables)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "\u001b[31mmatplotlib 1.3.1 requires nose, which is not installed.\u001b[0m\n",
      "Installing collected packages: numpy, numexpr, funcsigs, mock, tables\n",
      "  Found existing installation: numpy 1.8.0rc1\n",
      "\u001b[31mCannot uninstall 'numpy'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Requirement already satisfied: numpy in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (1.8.0rc1)\n",
      "Collecting SimpleITK\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/be/2ba6d932b7318937d5ae115f27f1915ef158cb61cba63772515b46499d3e/SimpleITK-1.2.4-cp27-cp27m-macosx_10_6_x86_64.whl\n",
      "Collecting tqdm\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 1.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting xlrd\n",
      "  Using cached https://files.pythonhosted.org/packages/b0/16/63576a1a001752e34bf8ea62e367997530dc553b689356b9879339cf45a4/xlrd-1.2.0-py2.py3-none-any.whl\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/52/ff/912fe03a623a70bcf297d466013a0b4f4c68c3b60f86bf226682d061fc09/pandas-0.24.2-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Requirement already satisfied: progressbar in /Users/danielnoble/Library/Python/2.7/lib/python/site-packages (2.5)\n",
      "Requirement already satisfied: matplotlib in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (1.3.1)\n",
      "Collecting nilearn\n",
      "  Using cached https://files.pythonhosted.org/packages/6e/65/ba76e7cd544dafc28960e60b099d6f906a2096034c560158beaf2ff299bc/nilearn-0.5.2-py2.py3-none-any.whl\n",
      "Collecting sklearn\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: pytz>=2011k in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from pandas) (2013.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/danielnoble/Library/Python/2.7/lib/python/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: tornado in /Users/danielnoble/Library/Python/2.7/lib/python/site-packages (from matplotlib) (5.1.1)\n",
      "Requirement already satisfied: pyparsing>=1.5.6 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from matplotlib) (2.0.1)\n",
      "Collecting nose (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/99/4f/13fb671119e65c4dce97c60e67d3fd9e6f7f809f2b307e2611f4701205cb/nose-1.3.7-py2-none-any.whl\n",
      "Collecting nibabel>=2.0.2 (from nilearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/16/53/258afe4a2415e35a78ea0800e98498922b17de9d03c394f5d60f28ba3eaf/nibabel-2.5.2.tar.gz\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  Complete output from command /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python /Library/Python/2.7/site-packages/pip-19.0.1-py2.7.egg/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /var/folders/xn/yfwfzxps1l70gvxtwg_7l4zh0000gn/T/tmpup93kj:\n",
      "  Traceback (most recent call last):\n",
      "    File \"/Library/Python/2.7/site-packages/pip-19.0.1-py2.7.egg/pip/_vendor/pep517/_in_process.py\", line 207, in <module>\n",
      "      main()\n",
      "    File \"/Library/Python/2.7/site-packages/pip-19.0.1-py2.7.egg/pip/_vendor/pep517/_in_process.py\", line 197, in main\n",
      "      json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "    File \"/Library/Python/2.7/site-packages/pip-19.0.1-py2.7.egg/pip/_vendor/pep517/_in_process.py\", line 54, in get_requires_for_build_wheel\n",
      "      return hook(config_settings)\n",
      "    File \"/private/var/folders/xn/yfwfzxps1l70gvxtwg_7l4zh0000gn/T/pip-build-env-8aYlMw/overlay/lib/python2.7/site-packages/setuptools/build_meta.py\", line 146, in get_requires_for_build_wheel\n",
      "      return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "    File \"/private/var/folders/xn/yfwfzxps1l70gvxtwg_7l4zh0000gn/T/pip-build-env-8aYlMw/overlay/lib/python2.7/site-packages/setuptools/build_meta.py\", line 127, in _get_build_requires\n",
      "      self.run_setup()\n",
      "    File \"/private/var/folders/xn/yfwfzxps1l70gvxtwg_7l4zh0000gn/T/pip-build-env-8aYlMw/overlay/lib/python2.7/site-packages/setuptools/build_meta.py\", line 142, in run_setup\n",
      "      exec(compile(code, __file__, 'exec'), locals())\n",
      "    File \"setup.py\", line 20, in <module>\n",
      "      from nisext.sexts import get_comrec_build, read_vars_from\n",
      "  ImportError: No module named nisext.sexts\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31mCommand \"/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python /Library/Python/2.7/site-packages/pip-19.0.1-py2.7.egg/pip/_vendor/pep517/_in_process.py get_requires_for_build_wheel /var/folders/xn/yfwfzxps1l70gvxtwg_7l4zh0000gn/T/tmpup93kj\" failed with error code 1 in /private/var/folders/xn/yfwfzxps1l70gvxtwg_7l4zh0000gn/T/pip-install-5HVL94/nibabel\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting git+https://www.github.com/farizrahman4u/keras-contrib.git\n",
      "  Cloning https://www.github.com/farizrahman4u/keras-contrib.git to /private/var/folders/xn/yfwfzxps1l70gvxtwg_7l4zh0000gn/T/pip-req-build-7eriv9\n",
      "Collecting keras (from keras-contrib==2.0.8)\n",
      "  Downloading https://files.pythonhosted.org/packages/44/e1/dc0757b20b56c980b5553c1b5c4c32d378c7055ab7bfa92006801ad359ab/Keras-2.4.3-py2.py3-none-any.whl\n",
      "Collecting pyyaml (from keras->keras-contrib==2.0.8)\n",
      "Collecting h5py (from keras->keras-contrib==2.0.8)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/47/e0d58be6f292684a4541d10b1da953542ff679f3ffc6096bee73634832b1/h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl\n",
      "Collecting scipy>=0.14 (from keras->keras-contrib==2.0.8)\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/7a/3de1700fb471a6f56353446b4b962a7e4a5baaf24e5e4dec2d91d5f500da/scipy-1.2.3-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting numpy>=1.9.1 (from keras->keras-contrib==2.0.8)\n",
      "  Using cached https://files.pythonhosted.org/packages/09/96/84cf406fe7d589f3dba9fc0f737e65985a3526c6d8c783f02d4b5a10825d/numpy-1.16.6-cp27-cp27m-macosx_10_9_x86_64.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from h5py->keras->keras-contrib==2.0.8) (1.12.0)\n",
      "Building wheels for collected packages: keras-contrib\n",
      "  Building wheel for keras-contrib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/xn/yfwfzxps1l70gvxtwg_7l4zh0000gn/T/pip-ephem-wheel-cache-jJRLY1/wheels/1f/8e/ac/fb1cca9d92276d64365b204e82a9a5bec1f24a20aca28fdbec\n",
      "Successfully built keras-contrib\n",
      "\u001b[31mmatplotlib 1.3.1 requires nose, which is not installed.\u001b[0m\n",
      "Installing collected packages: pyyaml, numpy, h5py, scipy, keras, keras-contrib\n",
      "\u001b[31mCould not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/PyYAML-5.3.1.dist-info'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting tensorflow==1.11.0\n",
      "  Using cached https://files.pythonhosted.org/packages/c1/d4/623b04520c344bd4b861efe89c83cef1f2d0d24d6791704d15920a1abd4a/tensorflow-1.11.0-cp27-cp27m-macosx_10_11_x86_64.whl\n",
      "Collecting protobuf>=3.6.0 (from tensorflow==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/bc/c97ae02fe3671035544f824b88005ca5f4a7e0c993aceb3fc705bbf949c9/protobuf-3.13.0-cp27-cp27m-macosx_10_9_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 1.3MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from tensorflow==1.11.0) (1.12.0)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/9a/d3afbb309896ee36b3e26cebd0e7c74164b18955348c1920737316c917af/grpcio-1.31.0-cp27-cp27m-macosx_10_9_x86_64.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 2.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow==1.11.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/4a/07c7e59cef23fb147454663c3271c21da68ba2ab141427c20548ae5a8a4d/gast-0.4.0.tar.gz\n",
      "Requirement already satisfied: enum34>=1.1.6 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from tensorflow==1.11.0) (1.1.6)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/7c/1d9fa17c363b5ff395cc6f5fd03219b9d303f31268983325974570d0d500/absl-py-0.10.0.tar.gz (110kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 1.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting backports.weakref>=1.0rc1 (from tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wheel in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from tensorflow==1.11.0) (0.33.1)\n",
      "Collecting keras-applications>=1.0.5 (from tensorflow==1.11.0)\n",
      "Collecting keras-preprocessing>=1.0.3 (from tensorflow==1.11.0)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 6.8MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting numpy>=1.13.3 (from tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/09/96/84cf406fe7d589f3dba9fc0f737e65985a3526c6d8c783f02d4b5a10825d/numpy-1.16.6-cp27-cp27m-macosx_10_9_x86_64.whl\n",
      "Collecting mock>=2.0.0 (from tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Collecting tensorboard<1.12.0,>=1.11.0 (from tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/76/f9/e62022d00940e4df9a629d6bfe42eb28907bb35808db62bb9e8b69ea5ef3/tensorboard-1.11.0-py2-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==1.11.0)\n",
      "Collecting setuptools<=39.1.0 (from tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: futures>=2.2.0; python_version < \"3.2\" in /Users/danielnoble/Library/Python/2.7/lib/python/site-packages (from grpcio>=1.8.6->tensorflow==1.11.0) (3.3.0)\n",
      "Collecting h5py (from keras-applications>=1.0.5->tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/47/e0d58be6f292684a4541d10b1da953542ff679f3ffc6096bee73634832b1/h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock>=2.0.0->tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.10 (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0)\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: gast, absl-py\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/danielnoble/Library/Caches/pip/wheels/22/3c/5f/1d5ffd8824bd0ce8943fbb2d6cf1568d6e91c46b7345c13a36\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/danielnoble/Library/Caches/pip/wheels/b2/54/03/c600d980c7bd24c98f60c4033e930f5e26259882db17f19824\n",
      "Successfully built gast absl-py\n",
      "\u001b[31mmatplotlib 1.3.1 requires nose, which is not installed.\u001b[0m\n",
      "Installing collected packages: setuptools, protobuf, grpcio, gast, absl-py, backports.weakref, numpy, h5py, keras-applications, keras-preprocessing, funcsigs, mock, markdown, werkzeug, tensorboard, astor, termcolor, tensorflow\n",
      "  Found existing installation: setuptools 41.0.1\n",
      "    Uninstalling setuptools-41.0.1:\n",
      "\u001b[31mCould not install packages due to an EnvironmentError: [Errno 1] Operation not permitted: '/private/var/folders/xn/yfwfzxps1l70gvxtwg_7l4zh0000gn/T/pip-temp-IA1otX/easy_install.py'\n",
      "\u001b[0m\n",
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting keras==2.2.4\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from keras==2.2.4)\n",
      "Collecting numpy>=1.9.1 (from keras==2.2.4)\n",
      "  Using cached https://files.pythonhosted.org/packages/09/96/84cf406fe7d589f3dba9fc0f737e65985a3526c6d8c783f02d4b5a10825d/numpy-1.16.6-cp27-cp27m-macosx_10_9_x86_64.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras==2.2.4)\n",
      "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from keras==2.2.4) (1.12.0)\n",
      "Collecting h5py (from keras==2.2.4)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/47/e0d58be6f292684a4541d10b1da953542ff679f3ffc6096bee73634832b1/h5py-2.10.0-cp27-cp27m-macosx_10_6_intel.whl\n",
      "Collecting pyyaml (from keras==2.2.4)\n",
      "Collecting scipy>=0.14 (from keras==2.2.4)\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/7a/3de1700fb471a6f56353446b4b962a7e4a5baaf24e5e4dec2d91d5f500da/scipy-1.2.3-cp27-cp27m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mmatplotlib 1.3.1 requires nose, which is not installed.\u001b[0m\n",
      "Installing collected packages: numpy, h5py, keras-applications, keras-preprocessing, pyyaml, scipy, keras\n",
      "  Found existing installation: numpy 1.8.0rc1\n",
      "\u001b[31mCannot uninstall 'numpy'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install correct dependencies\n",
    "!pip install tables\n",
    "!pip install numpy SimpleITK tqdm xlrd pandas progressbar matplotlib nilearn sklearn\n",
    "!pip install git+https://www.github.com/farizrahman4u/keras-contrib.git\n",
    "!pip install tensorflow==1.11.0\n",
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4ef8bcf13262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# If this does not print 1.11.0 restart the runtime after executing the above. (Ctrl+M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "# If this does not print 1.11.0 restart the runtime after executing the above. (Ctrl+M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive # import drive from google colab\n",
    "\n",
    "ROOT = \"/content/drive\"     # default location for the drive\n",
    "print(ROOT)                 # print content of ROOT (Optional)\n",
    "drive.mount(ROOT)           # we mount the google drive at /content/drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile  # For faster extraction\n",
    "zfile = zipfile.ZipFile(\"/content/drive/Shared drives/Brats/brats19.zip\")\n",
    "zfile.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get in there\n",
    "%cd brats19/demo_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import preprocess\n",
    "import train_model\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from keras.layers import Flatten,Dense\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "from dev_tools.my_tools import my_makedirs\n",
    "\n",
    "config = dict()\n",
    "config['initilizer'] = True\n",
    "config['l2'] = True\n",
    "config['model_file'] = 'os_model.h5'\n",
    "config['feature_indices'] = [4,5,6,10,11,12,13]\n",
    "\n",
    "config['training_target_csv'] = os.path.abspath('content/drive/Shared drives/Brats/training_gtr_only.csv')\n",
    "config['training_source_csv'] = os.path.abspath('content/drive/Shared drives/Brats/survival_data.csv')\n",
    "config['training_npz'] = os.path.abspath('content/drive/Shared drives/Brats/training_mine_gtr_only.npz')\n",
    "\n",
    "config['val_target_csv'] = os.path.abspath('content/drive/Shared drives/Brats/val_gtr_only.csv')\n",
    "config['val_source_csv'] = os.path.abspath('content/drive/Shared drives/Brats/survival_evaluation.csv')\n",
    "config['val_npz'] = os.path.abspath('content/drive/Shared drives/Brats/val_mine_gtr_only.npz')\n",
    "\n",
    "# implement leave-one-out cross-validation\n",
    "def LOOCV(npz_file = '../data/os_data/training_mine_gtr_only.npz', feature_index = [4,5,6,10,11,12,13]):\n",
    "  xy_zip = np.load(npz_file)\n",
    "  train_data = xy_zip['arr_0']\n",
    "  train_targets = xy_zip['arr_1']\n",
    "  num_epochs = 800\n",
    "\n",
    "  if feature_index:\n",
    "    train_data = train_data[:, feature_index]\n",
    "\n",
    "  for i in range(len(train_data)):\n",
    "    trn_data = np.delete(train_data, i, axis = 0)\n",
    "    trn_targets = np.delete(train_targets, i, axis = 0)\n",
    "    val_data = train_data[i]\n",
    "    val_targets = val_data[i]\n",
    "    model = build_model((train_data.shape[1],))\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=10, min_lr=1e-8)\n",
    "    history = model.fit(trn_data, \n",
    "                        trn_targets,\n",
    "                        validation_data = (val_data, val_targets), \n",
    "                        epochs = num_epochs,\n",
    "                        batch_size = 5,\n",
    "                        verbose = 0,\n",
    "                        callbacks = [reduce_lr])\n",
    "    print(model.evaluate(val_data, val_targets))\n",
    "    val_lms = history.history['val_lms_acc']\n",
    "    train_lms = history.history['lms_acc']\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_loss = history.history['loss']\n",
    "    train_mae = history.history['mean_absolute_error']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(mae_history) + 1), val_loss,label='val_loss')\n",
    "    plt.plot(range(1, len(mae_history) + 1), train_loss,label='train_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.show()\n",
    "\n",
    "# implement k-fold cross-validation\n",
    "def cross_validation(npz_file = '../data/os_data/training_mine_gtr_only.npz', feature_index = [4,5,6,10,11,12,13]):\n",
    "  xy_zip = np.load(npz_file)\n",
    "  train_data = xy_zip['arr_0']\n",
    "  train_targets = xy_zip['arr_1']\n",
    "\n",
    "  if feature_index:\n",
    "    train_data = train_data[:, feature_index]\n",
    "  \n",
    "  k = 4\n",
    "  num_epochs = 800\n",
    "  num_per_fold = len(train_data) / k\n",
    "\n",
    "  for fold in range(k):\n",
    "    \n",
    "    start_idx = int(fold * num_per_fold)\n",
    "    end_idx = int((fold + 1) * num_per_fold)\n",
    "\n",
    "    trn_data = np.delete(train_data, slice(start_idx, end_idx), axis = 0)\n",
    "    trn_targets = np.delete(train_targets, slice(start_idx, end_idx), axis = 0)\n",
    "    val_data = train_data[start_idx : end_idx]\n",
    "    val_targets = train_targets[start_idx : end_idx]\n",
    "\n",
    "    model = build_model((train_data.shape[1],))\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=10, min_lr=1e-8)\n",
    "    history = model.fit(trn_data, \n",
    "                        trn_targets,\n",
    "                        validation_data = (val_data, val_targets), \n",
    "                        epochs = num_epochs,\n",
    "                        batch_size = 5,\n",
    "                        verbose = 0,\n",
    "                        callbacks = [reduce_lr])\n",
    "    print(model.evaluate(val_data, val_targets))\n",
    "    val_lms = history.history['val_lms_acc']\n",
    "    train_lms = history.history['lms_acc']\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_loss = history.history['loss']\n",
    "    train_mae = history.history['mean_absolute_error']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(mae_history) + 1), val_loss,label='val_loss')\n",
    "    plt.plot(range(1, len(mae_history) + 1), train_loss,label='train_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.show()\n",
    "\n",
    "# implement monte carlo \n",
    "def monte_carlo(npz_file = '../data/os_data/training_mine_gtr_only.npz', feature_index = [4,5,6,10,11,12,13]):\n",
    "  xy_zip = np.load(npz_file)\n",
    "  train_data = xy_zip['arr_0']\n",
    "  train_targets = xy_zip['arr_1']\n",
    "  num_epochs = 800\n",
    "\n",
    "  if feature_index:\n",
    "    train_data = train_data[:, feature_index]\n",
    "\n",
    "  n_mc_iters = 20\n",
    "  n_per_mc_iter = 20\n",
    "\n",
    "  for it in range(n_mc_iters):\n",
    "    rand_points = np.random.choice(len(train_data), n_per_mc_iter)\n",
    "\n",
    "    trn_data = np.delete(train_data, rand_points, axis = 0)\n",
    "    trn_targets = np.delete(train_targets, rand_points, axis = 0)\n",
    "    val_data = train_data[rand_points]\n",
    "    val_targets = train_targets[rand_points]\n",
    "\n",
    "    model = build_model((train_data.shape[1],))\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=10, min_lr=1e-8)\n",
    "    history = model.fit(trn_data, \n",
    "                        trn_targets,\n",
    "                        validation_data = (val_data, val_targets), \n",
    "                        epochs = num_epochs,\n",
    "                        batch_size = 5,\n",
    "                        verbose = 0,\n",
    "                        callbacks = [reduce_lr])\n",
    "    print(model.evaluate(val_data, val_targets))\n",
    "    val_lms = history.history['val_lms_acc']\n",
    "    train_lms = history.history['lms_acc']\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_loss = history.history['loss']\n",
    "    train_mae = history.history['mean_absolute_error']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(mae_history) + 1), val_loss,label='val_loss')\n",
    "    plt.plot(range(1, len(mae_history) + 1), train_loss,label='train_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.show()\n",
    "\n",
    "def lms_acc(y_true, y_pred):\n",
    "    '''\n",
    "    long-mid-short survivor accuracy\n",
    "    '''\n",
    "#     pdb.set_trace()\n",
    "    y_pred = K.reshape(y_pred,(-1,))\n",
    "    y_true = K.reshape(y_true,(-1,))\n",
    "    long_survivors = K.sum(K.cast(y_pred > 15*30,'int32') * K.cast(y_true > 15*30, 'int32'))\n",
    "    short_survivors = K.sum(K.cast(y_pred < 10*30,'int32') * K.cast(y_true < 10*30, 'int32'))\n",
    "    mid_survivors = K.sum(K.cast(y_pred < 15*30,'int32') * K.cast(y_pred > 10*30,'int32')\n",
    "                        * K.cast(y_true < 15*30,'int32') * K.cast(y_true > 10*30,'int32')) \n",
    "    return (long_survivors + short_survivors + mid_survivors)/K.shape(y_true)[0]\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "#     model.compile(optimizer='rmsprop', loss='mse', metrics=['mae','mse','acc'])\n",
    "#     model.compile(optimizer=RMSprop(lr=0.001), loss='mse', metrics=['mae', 'acc'])\n",
    "    model.compile(optimizer=Adam(lr=1e-4), loss='mse', metrics=['mae', 'mse','acc',lms_acc])\n",
    "    return model\n",
    "    \n",
    "\n",
    "def final_train(epochs=800, npz_file='../data/os_data/training_mine_gtr_only.npz',feature_index=[4,5,6,10,11,12,13]):\n",
    "    xy_zip = np.load(npz_file)\n",
    "    train_data = xy_zip['arr_0']\n",
    "    train_targets = xy_zip['arr_1']\n",
    "    \n",
    "    if feature_index:\n",
    "        train_data = train_data[:,feature_index]\n",
    "        \n",
    "    val_data = train_data[25: 50]\n",
    "    val_targets = train_targets[25:50]\n",
    "    \n",
    "    mean = train_data.mean(axis=0)\n",
    "    train_data -= mean\n",
    "    std = train_data.std(axis=0)\n",
    "    train_data /= std\n",
    "    \n",
    "    model = build_model((train_data.shape[1],))\n",
    "    \n",
    "    callbacks_list=[]\n",
    "    callbacks_list.append(ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=10, min_lr=1e-8))\n",
    "    callbacks_list.append(ModelCheckpoint(config['model_file'], save_best_only=True))\n",
    "    callbacks_list.append(EarlyStopping(patience=50))\n",
    "    history = model.fit(train_data, \n",
    "                        train_targets,\n",
    "                        validation_data=(val_data,val_targets),\n",
    "                        epochs=epochs, \n",
    "                        batch_size=5, \n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks_list\n",
    "                        )\n",
    "\n",
    "    print(model.evaluate(val_data,val_targets))\n",
    "    print('predicted values:', model.predict(val_data).reshape(-1,))\n",
    "    print('targeted values:', val_targets)\n",
    "#     pdb.set_trace()\n",
    "    \n",
    "    val_lms = history.history['val_lms_acc']\n",
    "    train_lms = history.history['lms_acc']\n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_loss = history.history['loss']\n",
    "    train_mae = history.history['mean_absolute_error']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(mae_history) + 1), mae_history,label='val_mae')\n",
    "    plt.plot(range(1, len(mae_history) + 1), train_mae,label='train_mae')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(mae_history) + 1), val_loss,label='val_loss')\n",
    "    plt.plot(range(1, len(mae_history) + 1), train_loss,label='train_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('mse')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(mae_history) + 1), val_lms,label='val_lms')\n",
    "    plt.plot(range(1, len(mae_history) + 1), train_lms,label='train_lms')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('lms acc')\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "def evaluate(npz_file='../data/os_data/training_mine_gtr_only.npz',feature_index=[4,5,6,10,11,12,13]):\n",
    "#     pdb.set_trace()\n",
    "    xy_zip = np.load(npz_file)\n",
    "    train_data = xy_zip['arr_0']\n",
    "    train_targets = xy_zip['arr_1']\n",
    "    if feature_index:\n",
    "        train_data = train_data[:,feature_index]\n",
    "    mean = train_data.mean(axis=0)\n",
    "    train_data -= mean\n",
    "    std = train_data.std(axis=0)\n",
    "    train_data /= std\n",
    "    val_data = train_data[25: 50]\n",
    "    val_targets = train_targets[25:50]\n",
    "    \n",
    "    model = load_model(config['model_file'],custom_objects={'lms_acc':lms_acc})\n",
    "    predicted = np.reshape(model.predict(val_data),(-1,))\n",
    "    \n",
    "    print('predicted survival days:',predicted)\n",
    "    print('val_targets:',val_targets)\n",
    "    print(model.evaluate(val_data,val_targets))\n",
    "    return predicted, val_targets\n",
    "\n",
    "def predict():\n",
    "#     pdb.set_trace()\n",
    "    xy_zip = np.load(config['training_npz'])\n",
    "    train_data = xy_zip['arr_0']\n",
    "    train_data = train_data[:,config['feature_indices']]\n",
    "    mean = train_data.mean(axis=0)\n",
    "    train_data -= mean\n",
    "    std = train_data.std(axis=0)\n",
    "    train_data /= std\n",
    "    \n",
    "    xy_zip = np.load(config['val_npz'])\n",
    "    val_data = xy_zip['arr_0']\n",
    "    val_data = val_data[:,config['feature_indices']]\n",
    "    val_data -= mean\n",
    "    val_data /= std\n",
    "    \n",
    "    model = load_model(config['model_file'],custom_objects={'lms_acc':lms_acc})\n",
    "    \n",
    "    train_predicted = model.predict(train_data).reshape(-1,)\n",
    "    val_predicted = model.predict(val_data).reshape(-1,)\n",
    "    \n",
    "#     print(train_predicted)\n",
    "#     print(val_predicted)\n",
    "    train_predicted = np.round(train_predicted).astype(int)\n",
    "    val_predicted = np.round(val_predicted).astype(int)\n",
    "    \n",
    "    my_makedirs('saves')\n",
    "    \n",
    "    df = pd.read_csv(config['training_target_csv'])\n",
    "    id_list = list(df['BraTS19ID'])\n",
    "    os_list = list(train_predicted)\n",
    "    data_to_save = pd.DataFrame({'id':id_list,'os':os_list})\n",
    "    data_to_save.to_csv(os.path.join('saves','training_to_upload.csv'), index=False, sep=',',header=False)\n",
    "    \n",
    "\n",
    "    # NOTE --- Using BraTS18ID for validation data.\n",
    "    #   We have the features extracted from the 2019 training data, but had\n",
    "    #   to generate the validation results ourselves on the 2018 data,\n",
    "    #   using the network pretrained on the '19 data. \n",
    "    df = pd.read_csv(config['val_target_csv'])\n",
    "    id_list = list(df['BraTS18ID'])\n",
    "    os_list = list(val_predicted)\n",
    "    data_to_save = pd.DataFrame({'id':id_list,'os':os_list})\n",
    "    data_to_save.to_csv(os.path.join('saves','val_to_upload.csv'), index=False, sep=',',header=False)\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #cross_validation()\n",
    "    #print(\"Cross validation\")\n",
    "\n",
    "    monte_carlo()\n",
    "    print(\"done\")\n",
    "    final_train()\n",
    "    evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile  # For faster extraction\n",
    "\n",
    "# train_path = \"/content/drive/Shared drives/Brats/MICCAI_BraTS_2018_Data_Training.zip\"\n",
    "# valid_path = \"/content/drive/Shared drives/Brats/MICCAI_BraTS_2018_Data_Validation.zip\"\n",
    "\n",
    "# # Create data directory\n",
    "# %mkdir data \n",
    "# %cd data \n",
    "\n",
    "# # Unzip training data\n",
    "# %mkdir original\n",
    "# %cd original\n",
    "# zfile = zipfile.ZipFile(train_path)\n",
    "# zfile.extractall()\n",
    "# #%mv survival_data.csv ../\n",
    "# %cd ../\n",
    "\n",
    "# # Unzip validation data\n",
    "# %mkdir val\n",
    "# %cd val\n",
    "# %mkdir val\n",
    "# %cd val\n",
    "# zfile = zipfile.ZipFile(valid_path)\n",
    "# zfile.extractall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
